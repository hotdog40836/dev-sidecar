{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTPlx8koxNkK",
        "outputId": "7d3c19b0-40d8-446b-b21f-e26e1be7a1c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-07-01 02:08:32--  https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/fxsjy/jieba/master/extra_dict/dict.txt.big [following]\n",
            "--2025-07-01 02:08:33--  https://raw.githubusercontent.com/fxsjy/jieba/master/extra_dict/dict.txt.big\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8583143 (8.2M) [application/octet-stream]\n",
            "Saving to: ‘dict.txt.big’\n",
            "\n",
            "dict.txt.big        100%[===================>]   8.18M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-07-01 02:08:33 (104 MB/s) - ‘dict.txt.big’ saved [8583143/8583143]\n",
            "\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "import jieba\n",
        "import jieba.analyse\n",
        "\n",
        "!apt-get install -qq fonts-noto-cjk\n",
        "!wget -O dict.txt.big https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big\n",
        "!wget -q https://github.com/google/fonts/raw/main/ofl/notosanstc/NotoSansTC-Regular.otf -O /content/NotoSansTC-Regular.otf\n",
        "!pip install --upgrade Pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsaiP4Jdra4c",
        "outputId": "c1ab307a-435c-43ed-da65-a03a14a5a812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded page 1 of job listings.\n",
            "Downloaded page 2 of job listings.\n",
            "Downloaded page 3 of job listings.\n",
            "Downloaded page 4 of job listings.\n",
            "Downloaded page 5 of job listings.\n",
            "Downloaded page 6 of job listings.\n",
            "Downloaded page 7 of job listings.\n",
            "Downloaded page 8 of job listings.\n",
            "Downloaded page 9 of job listings.\n",
            "Downloaded page 10 of job listings.\n",
            "Downloaded page 11 of job listings.\n",
            "Downloaded page 12 of job listings.\n",
            "Downloaded page 13 of job listings.\n",
            "Downloaded page 14 of job listings.\n",
            "Downloaded page 15 of job listings.\n",
            "Downloaded page 16 of job listings.\n",
            "Downloaded page 17 of job listings.\n",
            "Downloaded page 18 of job listings.\n",
            "Downloaded page 19 of job listings.\n",
            "Downloaded page 20 of job listings.\n",
            "Downloaded page 21 of job listings.\n",
            "Downloaded page 22 of job listings.\n",
            "Downloaded page 23 of job listings.\n",
            "Downloaded page 24 of job listings.\n",
            "Downloaded page 25 of job listings.\n",
            "Downloaded page 26 of job listings.\n",
            "Downloaded page 27 of job listings.\n",
            "Downloaded page 28 of job listings.\n",
            "Downloaded page 29 of job listings.\n",
            "Downloaded page 30 of job listings.\n",
            "Downloaded page 31 of job listings.\n",
            "Downloaded page 32 of job listings.\n",
            "Downloaded page 33 of job listings.\n",
            "Downloaded page 34 of job listings.\n",
            "Downloaded page 35 of job listings.\n",
            "Downloaded page 36 of job listings.\n",
            "Downloaded page 37 of job listings.\n",
            "Downloaded page 38 of job listings.\n",
            "Downloaded page 39 of job listings.\n",
            "Downloaded page 40 of job listings.\n",
            "Downloaded page 41 of job listings.\n",
            "Downloaded page 42 of job listings.\n",
            "Downloaded page 43 of job listings.\n",
            "Downloaded page 44 of job listings.\n",
            "Downloaded page 45 of job listings.\n",
            "Downloaded page 46 of job listings.\n",
            "Downloaded page 47 of job listings.\n",
            "Downloaded page 48 of job listings.\n",
            "Downloaded page 49 of job listings.\n",
            "Downloaded page 50 of job listings.\n",
            "Extracted job URLs from page 1.\n",
            "Extracted job URLs from page 2.\n",
            "Extracted job URLs from page 3.\n",
            "Extracted job URLs from page 4.\n",
            "Extracted job URLs from page 5.\n",
            "Extracted job URLs from page 6.\n",
            "Extracted job URLs from page 7.\n",
            "Extracted job URLs from page 8.\n",
            "Extracted job URLs from page 9.\n",
            "Extracted job URLs from page 10.\n",
            "Extracted job URLs from page 11.\n",
            "Extracted job URLs from page 12.\n",
            "Extracted job URLs from page 13.\n",
            "Extracted job URLs from page 14.\n",
            "Extracted job URLs from page 15.\n",
            "Extracted job URLs from page 16.\n",
            "Extracted job URLs from page 17.\n",
            "Extracted job URLs from page 18.\n",
            "Extracted job URLs from page 19.\n",
            "Extracted job URLs from page 20.\n",
            "Extracted job URLs from page 21.\n",
            "Extracted job URLs from page 22.\n",
            "Extracted job URLs from page 23.\n",
            "Extracted job URLs from page 24.\n",
            "Extracted job URLs from page 25.\n",
            "Extracted job URLs from page 26.\n",
            "Extracted job URLs from page 27.\n",
            "Extracted job URLs from page 28.\n",
            "Extracted job URLs from page 29.\n",
            "Extracted job URLs from page 30.\n",
            "Extracted job URLs from page 31.\n",
            "Extracted job URLs from page 32.\n",
            "Extracted job URLs from page 33.\n",
            "Extracted job URLs from page 34.\n",
            "Extracted job URLs from page 35.\n",
            "Extracted job URLs from page 36.\n",
            "Extracted job URLs from page 37.\n",
            "Extracted job URLs from page 38.\n",
            "Extracted job URLs from page 39.\n",
            "Extracted job URLs from page 40.\n",
            "Extracted job URLs from page 41.\n",
            "Extracted job URLs from page 42.\n",
            "Extracted job URLs from page 43.\n",
            "Extracted job URLs from page 44.\n",
            "Extracted job URLs from page 45.\n",
            "Extracted job URLs from page 46.\n",
            "Extracted job URLs from page 47.\n",
            "Extracted job URLs from page 48.\n",
            "Extracted job URLs from page 49.\n",
            "Extracted job URLs from page 50.\n",
            "Total job URLs extracted: 67\n",
            "Downloaded job description 1 of 67\n",
            "Downloaded job description 2 of 67\n",
            "Downloaded job description 3 of 67\n",
            "Downloaded job description 4 of 67\n",
            "Downloaded job description 5 of 67\n",
            "Downloaded job description 6 of 67\n",
            "Downloaded job description 7 of 67\n",
            "Downloaded job description 8 of 67\n",
            "Downloaded job description 9 of 67\n",
            "Downloaded job description 10 of 67\n",
            "Downloaded job description 11 of 67\n",
            "Downloaded job description 12 of 67\n",
            "Downloaded job description 13 of 67\n",
            "Downloaded job description 14 of 67\n",
            "Downloaded job description 15 of 67\n",
            "Downloaded job description 16 of 67\n",
            "Downloaded job description 17 of 67\n",
            "Downloaded job description 18 of 67\n",
            "Downloaded job description 19 of 67\n",
            "Downloaded job description 20 of 67\n",
            "Downloaded job description 21 of 67\n",
            "Downloaded job description 22 of 67\n",
            "Downloaded job description 23 of 67\n",
            "Downloaded job description 24 of 67\n",
            "Downloaded job description 25 of 67\n",
            "Downloaded job description 26 of 67\n",
            "Downloaded job description 27 of 67\n",
            "Downloaded job description 28 of 67\n",
            "Downloaded job description 29 of 67\n",
            "Downloaded job description 30 of 67\n",
            "Downloaded job description 31 of 67\n",
            "Downloaded job description 32 of 67\n",
            "Downloaded job description 33 of 67\n",
            "Downloaded job description 34 of 67\n",
            "Downloaded job description 35 of 67\n",
            "Downloaded job description 36 of 67\n",
            "Downloaded job description 37 of 67\n",
            "Downloaded job description 38 of 67\n",
            "Downloaded job description 39 of 67\n",
            "Downloaded job description 40 of 67\n",
            "Downloaded job description 41 of 67\n",
            "Downloaded job description 42 of 67\n",
            "Downloaded job description 43 of 67\n",
            "Downloaded job description 44 of 67\n",
            "Downloaded job description 45 of 67\n",
            "Downloaded job description 46 of 67\n",
            "Downloaded job description 47 of 67\n",
            "Downloaded job description 48 of 67\n",
            "Downloaded job description 49 of 67\n",
            "Downloaded job description 50 of 67\n",
            "Downloaded job description 51 of 67\n",
            "Downloaded job description 52 of 67\n",
            "Downloaded job description 53 of 67\n",
            "Downloaded job description 54 of 67\n",
            "Downloaded job description 55 of 67\n",
            "Downloaded job description 56 of 67\n",
            "Downloaded job description 57 of 67\n",
            "Downloaded job description 58 of 67\n",
            "Downloaded job description 59 of 67\n",
            "Downloaded job description 60 of 67\n",
            "Downloaded job description 61 of 67\n",
            "Downloaded job description 62 of 67\n",
            "Downloaded job description 63 of 67\n",
            "Downloaded job description 64 of 67\n",
            "Downloaded job description 65 of 67\n",
            "Downloaded job description 66 of 67\n",
            "Downloaded job description 67 of 67\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-5-2211333262.py:99: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  work_experience_element = soup.find(\"h3\", text=\"工作經歷\")\n",
            "/tmp/ipython-input-5-2211333262.py:107: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  education_requirements_element = soup.find(\"h3\", text=\"學歷要求\")\n",
            "/tmp/ipython-input-5-2211333262.py:115: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  tools_element = soup.find(\"h3\", text=\"擅長工具\")\n",
            "/tmp/ipython-input-5-2211333262.py:123: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  skills_element = soup.find(\"h3\", text=\"工作技能\")\n",
            "/tmp/ipython-input-5-2211333262.py:131: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  other_conditions_element = soup.find(\"h3\", text=\"其他條件\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed job description for: AI生成服務專案應用經理\n",
            "Processed job description for: AI影像設計\n",
            "Processed job description for: 【知識】平台營運部｜策略聯盟經理 Affiliate Manager\n",
            "Processed job description for: FAE & AI 應用規劃工程師  Field & AI application Engineer\n",
            "Processed job description for: 【台北內科】Backend Engineer, AI & Legal Automation (Python/PHP) AI法律自動化後端工程師\n",
            "Processed job description for: AI行銷企劃\n",
            "Processed job description for: 視覺事業處_AI Agent & Vision Engineer (AI代理工程師)\n",
            "Processed job description for: 數位轉型管理師\n",
            "Processed job description for: HQ－（Sr.）數位轉型技術經理（汐止總部）\n",
            "Processed job description for: Staff Engineer\n",
            "Processed job description for: 自動化內容實習生｜AI 工具 /N8N\n",
            "Processed job description for: 《市場營銷處》數據產品經理（Data Product Manager）｜網站營運 × 數據分析 × AI應用\n",
            "Processed job description for: AI 整合顧問｜自動化流程設計 / 跨雲部署 / 生成式 AI 專案\n",
            "Processed job description for: 【北士科】Full Stack Developer, AI LegalTech Platform (Python/PHP) AI法律科技平台全端工程師\n",
            "Processed job description for: 【點點印】高階主管特助（溝通｜數據分析｜AI 工具）\n",
            "Processed job description for: 雲端資安系統/助理工程師\n",
            "Processed job description for: ◎ AI流程開發工程師\n",
            "Processed job description for: 【總公司】數據工程師\n",
            "Processed job description for: AI 應用工程師\n",
            "Processed job description for: 資訊系統開發工程師\n",
            "Processed job description for: SL[台北/台中/新竹] 跨國軟體科技公司-AI Engineer (熟Python / NLP)\n",
            "Processed job description for: D01-自動化工程師(AE)_旅遊科技部_技術研發組\n",
            "Processed job description for: 【Ocard顧客經營管家】AI 工程師 (擴編招募中)\n",
            "Processed job description for: 【新竹】Backend Engineer, AI & Legal Automation (Python/PHP) AI法律自動化後端工程師\n",
            "Processed job description for: AI 開發工程師 (Vibe Coding Developer)\n",
            "Processed job description for: 【台中】Full Stack Developer, AI LegalTech Platform (Python/PHP) AI法律科技平台全端工程師\n",
            "Processed job description for: AI 自動化流程工程師\n",
            "Processed job description for: 軟體開發工程師\n",
            "Processed job description for: 【北士科】Backend Engineer, AI & Legal Automation (Python/PHP) AI法律自動化後端工程師\n",
            "Processed job description for: 總部功能 資安資訊中心-SAP ERP SD 系統工程師(內湖)\n",
            "Processed job description for: Backend Product Engineer【企業 AI 應用開發】\n",
            "Processed job description for: AI 開發工程師\n",
            "Processed job description for: 自動化流程工程師 Automation Process Engineer\n",
            "Processed job description for: Director of Engineering 技術總監\n",
            "Processed job description for: 【北士科】Software Engineer, Legal Automation & AI Integration (Python/PHP) 法律自動化與AI整合軟體工程師\n",
            "Processed job description for: 【台北內科】Software Engineer, Legal Automation & AI Integration (Python/PHP) 法律自動化與AI整合軟體工程師\n",
            "Processed job description for: 內容商務經理 Delivery Manager\n",
            "Processed job description for: 【資深資料分析師】Senior Data Analyst\n",
            "Processed job description for: AI 內容行銷與自動化專家\n",
            "Processed job description for: AI(助理)工程師\n",
            "Processed job description for: 【新竹】Full Stack Developer, AI LegalTech Platform (Python/PHP) AI法律科技平台全端工程師\n",
            "Processed job description for: 【台北】SA系統分析師(保障年薪13個月)-部分遠端\n",
            "Processed job description for: 實習生_台中資訊\n",
            "Processed job description for: MIS\n",
            "Processed job description for: Software Engineer, Automation & Engineering Platform_軟體工程師－自動化與工程平台\n",
            "Processed job description for: 【南港辦公室】AI 應用工程師-1\n",
            "Processed job description for: 社群與產品內容助理\n",
            "Processed job description for: 【台北內科】Full Stack Developer, AI LegalTech Platform (Python/PHP) AI法律科技平台全端工程師\n",
            "Processed job description for: AI 實習生 (PLM/ALM 解決方案)\n",
            "Processed job description for: Full Stack Engineer 【AI 應用整合開發】\n",
            "Processed job description for: 總部功能 資安資訊中心-SAP ERP MM/PP 系統工程師(內湖)\n",
            "Processed job description for: 【2025數位永續種子】AI流程自動化實習生 AI Process Automation Intern (台南/Tainan)\n",
            "Processed job description for: Windows 軟體開發工程師（AI 整合＆輸入法開發 ）\n",
            "Processed job description for: Senior AI System Architecture AI系統架構師\n",
            "Processed job description for: [HQ - Taipei] 營運優化實習生\n",
            "Processed job description for: 法律 AI 解決方案工程師\n",
            "Processed job description for: 新創科技｜行政會計｜擴編\n",
            "Processed job description for: macOS 軟體開發工程師（AI 整合＆輸入法開發 ）\n",
            "Processed job description for: 【台中】Software Engineer, Legal Automation & AI Integration (Python/PHP) 法律自動化與AI整合軟體工程師\n",
            "Processed job description for: 【台中】Backend Engineer, AI & Legal Automation (Python/PHP) AI法律自動化後端工程師\n",
            "Processed job description for: 【擴編-無經驗可】行政專員\n",
            "Processed job description for: 美編設計｜茶葉電商品牌\n",
            "Processed job description for: Senior AI / ML Engineer\n",
            "Processed job description for: 【新竹】Software Engineer, Legal Automation & AI Integration (Python/PHP) 法律自動化與AI整合軟體工程師\n",
            "Processed job description for: 【台北】Junior SA系統分析師(保障年薪13個月)-部分遠端\n",
            "Processed job description for: 軟體工程師\n",
            "Processed job description for: Director/Staff Software Engineer 軟體技術總監\n",
            "Data saved to job_data_n8n.csv\n"
          ]
        }
      ],
      "source": [
        "# 定義函數下載搜尋結果前五頁的網頁檔案\n",
        "def download_first_five_pages():\n",
        "    directory = \"/content/job_listings\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    for page_number in range(1, 51):\n",
        "        html_to_save = f\"https://www.104.com.tw/jobs/search/?jobsource=index_s&keyword=n8n&mode=s&page={page_number}&order=15\"\n",
        "        r = requests.get(html_to_save)\n",
        "        with open(f\"{directory}/search_result_page_{page_number}.html\", \"w\") as file:\n",
        "            file.write(r.text)\n",
        "        print(f\"Downloaded page {page_number} of job listings.\")\n",
        "\n",
        "# 定義函數擷取前五頁搜尋結果的職缺連結\n",
        "def get_job_url_list() -> list:\n",
        "    directory = \"/content/job_listings\"\n",
        "    job_title_css_selector = \"h2 > a\"\n",
        "    job_url_list = []\n",
        "\n",
        "    for page_number in range(1, 51):\n",
        "        with open(f\"{directory}/search_result_page_{page_number}.html\") as file:\n",
        "            soup = BeautifulSoup(file, 'html.parser')\n",
        "        job_url_hrefs = [elem.get(\"href\") for elem in soup.select(job_title_css_selector)]\n",
        "        job_urls = [job_url_href if job_url_href.startswith(\"https\") else f\"https:{job_url_href}\" for job_url_href in job_url_hrefs if \"hotjob_chr\" not in job_url_href]\n",
        "        job_url_list += job_urls\n",
        "        print(f\"Extracted job URLs from page {page_number}.\")\n",
        "\n",
        "    print(f\"Total job URLs extracted: {len(job_url_list)}\")\n",
        "    return job_url_list\n",
        "\n",
        "# 定義函數下載職缺連結的網頁檔案\n",
        "def download_job_descriptions(job_url_list: list):\n",
        "    directory = \"/content/job_listings/job_descriptions\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    for idx, job_url in enumerate(job_url_list):\n",
        "        r = requests.get(job_url)\n",
        "        job_url_split = job_url.split(\"?\")[0]\n",
        "        page_name = job_url_split.split(\"/\")[-1]\n",
        "        with open(f\"{directory}/{page_name}.html\", \"w\") as file:\n",
        "            file.write(r.text)\n",
        "        print(f\"Downloaded job description {idx + 1} of {len(job_url_list)}\")\n",
        "\n",
        "        # 增加等待時間以降低爬取速度\n",
        "        time.sleep(1)  # 每爬取一次職缺頁面後等待 2 秒\n",
        "\n",
        "\n",
        "# 執行函數\n",
        "download_first_five_pages()\n",
        "job_url_list = get_job_url_list()\n",
        "download_job_descriptions(job_url_list)\n",
        "\n",
        "\n",
        "# 擷取職缺的工作敘述\n",
        "directory = \"/content/job_listings/job_descriptions\"\n",
        "list_dir = os.listdir(directory)\n",
        "job_titles, employers, job_locations, salaries, job_descriptions = [], [], [], [], []\n",
        "work_experience_list, education_requirements_list, tools_list, skills_list, other_conditions_list = [], [], [], [], []\n",
        "\n",
        "for html_file in list_dir:\n",
        "    with open(f\"{directory}/{html_file}\") as file:\n",
        "        soup = BeautifulSoup(file, 'html.parser')\n",
        "\n",
        "    # 職缺標題\n",
        "    #job_title_element = soup.select(\"div.job-header__title > h1\")\n",
        "    job_title_element = soup.select(\"hgroup > h1, div.job-header__title > h1\")\n",
        "    job_title = job_title_element[0].text.strip() if job_title_element else \"N/A\"\n",
        "\n",
        "    # 公司名稱\n",
        "    employer_element = soup.select(\"div.job-header__title a[data-gtm-head='公司名稱']\")\n",
        "    if employer_element:\n",
        "        employer = employer_element[0].text.strip()\n",
        "    else:\n",
        "        employer = \"N/A\"\n",
        "\n",
        "    # 工作地點\n",
        "    job_location_element = soup.find(\"div\", class_=\"job-address\")\n",
        "    if job_location_element and job_location_element.has_attr(\"addressarea\"):\n",
        "        job_location = job_location_element[\"addressarea\"].strip()\n",
        "    else:\n",
        "        job_location = \"N/A\"\n",
        "\n",
        "    # 薪資範圍\n",
        "    salary_element = soup.select(\"div.list-row__data p.text-primary.font-weight-bold\")\n",
        "    if salary_element:\n",
        "        salary = salary_element[0].text.strip()\n",
        "    else:\n",
        "        salary = \"N/A\"\n",
        "\n",
        "    # 工作描述\n",
        "    job_description_element = soup.select(\"p.job-description__content\")\n",
        "    if job_description_element:\n",
        "        job_description = \" \".join([elem.text.strip() for elem in job_description_element])\n",
        "    else:\n",
        "      job_description = \"N/A\"\n",
        "\n",
        "    # 工作經歷\n",
        "    work_experience_element = soup.find(\"h3\", text=\"工作經歷\")\n",
        "    if work_experience_element:\n",
        "        work_experience_data = work_experience_element.find_next(\"div\", class_=\"t3\")\n",
        "        work_experience = work_experience_data.text.strip() if work_experience_data else \"N/A\"\n",
        "    else:\n",
        "        work_experience = \"N/A\"\n",
        "\n",
        "    # 學歷要求\n",
        "    education_requirements_element = soup.find(\"h3\", text=\"學歷要求\")\n",
        "    if education_requirements_element:\n",
        "        education_data = education_requirements_element.find_next(\"div\", class_=\"t3\")\n",
        "        education_requirements = education_data.text.strip() if education_data else \"N/A\"\n",
        "    else:\n",
        "        education_requirements = \"N/A\"\n",
        "\n",
        "    # 擅長工具\n",
        "    tools_element = soup.find(\"h3\", text=\"擅長工具\")\n",
        "    if tools_element:\n",
        "        tools_data = tools_element.find_next(\"div\", class_=\"t3\")\n",
        "        tools = tools_data.text.strip() if tools_data else \"N/A\"\n",
        "    else:\n",
        "        tools = \"N/A\"\n",
        "\n",
        "    # 工作技能\n",
        "    skills_element = soup.find(\"h3\", text=\"工作技能\")\n",
        "    if skills_element:\n",
        "        skills_data = skills_element.find_next(\"div\", class_=\"t3\")\n",
        "        skills = skills_data.text.strip() if skills_data else \"N/A\"\n",
        "    else:\n",
        "        skills = \"N/A\"\n",
        "\n",
        "    # 其他條件\n",
        "    other_conditions_element = soup.find(\"h3\", text=\"其他條件\")\n",
        "    if other_conditions_element:\n",
        "        other_conditions_data = other_conditions_element.find_next(\"p\")\n",
        "        other_conditions = other_conditions_data.text.strip() if other_conditions_data else \"N/A\"\n",
        "    else:\n",
        "        other_conditions = \"N/A\"\n",
        "\n",
        "    # 將提取到的資料加入列表\n",
        "    job_titles.append(job_title)\n",
        "    employers.append(employer)\n",
        "    job_locations.append(job_location)\n",
        "    salaries.append(salary)\n",
        "    job_descriptions.append(job_description)\n",
        "    work_experience_list.append(work_experience)\n",
        "    education_requirements_list.append(education_requirements)\n",
        "    tools_list.append(tools)\n",
        "    skills_list.append(skills)\n",
        "    other_conditions_list.append(other_conditions)\n",
        "\n",
        "    print(f\"Processed job description for: {job_title}\")\n",
        "\n",
        "# 建立 DataFrame\n",
        "df = pd.DataFrame({\n",
        "    \"job_title\": job_titles,\n",
        "    \"employer\": employers,\n",
        "    \"location\": job_locations,\n",
        "    \"salary\": salaries,\n",
        "    \"description\": job_descriptions,\n",
        "    \"experience\": work_experience_list,\n",
        "    \"education\": education_requirements_list,\n",
        "    \"tools\": tools_list,\n",
        "    \"skills\": skills_list,\n",
        "    \"conditions\": other_conditions_list\n",
        "})\n",
        "\n",
        "\n",
        "# 將資料保存為 CSV 檔案\n",
        "df.to_csv(\"/content/job_data_n8n(0701).csv\", index=False)\n",
        "print(\"Data saved to job_data_n8n.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "h35qShUJH-v9",
        "outputId": "c754870e-c07a-4619-f137-cd82efdf764b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5011690d-2eb1-4116-842b-380c95fe0fa7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5011690d-2eb1-4116-842b-380c95fe0fa7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-1100401757.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 讓使用者上傳檔案\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 取得檔案名稱\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     result = _output.eval_js(\n\u001b[1;32m    173\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "# 讓使用者上傳檔案\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 取得檔案名稱\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# 讀取 Excel 檔案\n",
        "df = pd.read_excel(file_name) # 這裡使用您剛剛上傳的檔案名稱"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_mP_MqixEypM"
      },
      "outputs": [],
      "source": [
        "font_path = \"/content/NotoSansTC-Regular.ttf\"\n",
        "font = ImageFont.truetype(font_path, size=40)\n",
        "\n",
        "# 創建一個空白圖像 (RGB 模式，寬 400 像素，高 200 像素，背景色為白色)\n",
        "image = Image.new(\"RGB\", (900, 200), (500, 200, 100))\n",
        "draw = ImageDraw.Draw(image)\n",
        "\n",
        "# 在圖像上繪製中文文字\n",
        "text = \"胡仁總灌君！\"\n",
        "draw.text((50, 50), text, font=font, fill=(0, 0, 0))\n",
        "\n",
        "# 顯示圖像\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow(image)\n",
        "plt.axis('off')  # 關閉座標軸顯示\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uBKmgmuv8njq"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Step 1: 從 Excel 中讀取資料（假設 df 已經在環境中可用）\n",
        "# -------------------------------\n",
        "# 提取 description 和 conditions 欄位，並合併文字\n",
        "def get_text_from_df(df):\n",
        "    description_text = ' '.join(df['description'].dropna())\n",
        "    skills_text = ' '.join(df['skills'].dropna())\n",
        "    conditions_text = ' '.join(df['conditions'].dropna())\n",
        "    tools_text = ' '.join(df[[f\"tools.{i}\" for i in range(1, 18)]].stack().astype(str))\n",
        "    title_text = ' '.join(df['job_title'].dropna())\n",
        "    return title_text\n",
        "    #return tools_text\n",
        "\n",
        "text = get_text_from_df(df)\n",
        "\n",
        "# -------------------------------\n",
        "# Step 2: 下載 Jieba 的繁體字典（若檔案不存在才下載）\n",
        "# -------------------------------\n",
        "dict_path = 'dict.txt.big'\n",
        "if not os.path.exists(dict_path):\n",
        "    !wget -O {dict_path} https://raw.githubusercontent.com/fxsjy/jieba/master/extra_dict/dict.txt.big\n",
        "\n",
        "# -------------------------------\n",
        "# Step 3: 設定 jieba 字典並進行斷詞\n",
        "# -------------------------------\n",
        "jieba.set_dictionary(dict_path)\n",
        "seg_list = jieba.lcut(text, cut_all=False)\n",
        "\n",
        "# -------------------------------\n",
        "# Step 4: 關鍵詞抽取（取前 10 個）\n",
        "# -------------------------------\n",
        "keywords = jieba.analyse.extract_tags(text, topK=15, withWeight=False)\n",
        "print(f\"關鍵詞：{keywords}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Step 5: 使用 pandas 計算詞頻\n",
        "# 定義停用詞集合，過濾無意義的詞彙（可依需求擴充）\n",
        "# 定義連結詞變體（常見的及、與、和、等各種全形、半形形式）\n",
        "linking_words_variants = {\n",
        "    \"及\", \"及,\", \"及，\", \"及.\", \"及．\", \"及…\",\n",
        "    \",及\", \"，及\", \"、及\", \".及\", \"．及\", \"…及\",\n",
        "    \"與\", \"與,\", \"與，\", \"與.\", \"與．\", \"與…\",\n",
        "    \",與\", \"，與\", \"、與\", \".與\", \"．與\", \"…與\",\n",
        "    \"和\", \"和,\", \"和，\", \"和.\", \"和．\", \"和…\",\n",
        "    \",和\", \"，和\", \"、和\", \".和\", \"．和\", \"…和\",\n",
        "    \"等\", \"等,\", \"等，\", \"等.\", \"等．\", \"等…\",\n",
        "    \",等\", \"，等\", \"、等\", \".等\", \"．等\", \"…等\",\n",
        "}\n",
        "stopwords = set([\n",
        "    '，', ',', '。', '.', '、', '；', ';', '：', ':', '！', '!', '？', '?', '…', '⋯',\n",
        "    '的', '了', '在', '是', '和', '及', '與', '或', '但', '而', '並', '且', '等',\n",
        "    '這', '那', '一', '個', '一些', '每', '每個', '請', '可', '可能', '以上', '以下',\n",
        "    '以及', '其中', '相關', '符合', '任職'\n",
        "]) | linking_words_variants\n",
        "\n",
        "def count_segment_freq(seg_list):\n",
        "    seg_df = pd.DataFrame(seg_list, columns=['seg'])\n",
        "    seg_df = seg_df[~seg_df['seg'].isin(stopwords)]\n",
        "    seg_df['count'] = 1\n",
        "    seg_freq = seg_df.groupby('seg')['count'].sum().sort_values(ascending=False)\n",
        "    return seg_freq\n",
        "\n",
        "seg_freq = count_segment_freq(seg_list)\n",
        "print(\"詞頻統計前十五：\")\n",
        "print(seg_freq.head(15))\n",
        "\n",
        "# 示範如何使用：\n",
        "# 假設您已有斷詞結果 seg_list 是一個詞彙列表，且您知道當前處理的是哪個欄位（例如 job_title）\n",
        "current_field = \"job_title\"  # 可依實際欄位調整\n",
        "# 過濾掉該欄位的停用詞\n",
        "#filtered_seg_list = [word for word in seg_list if word not in stopwords_by_field[current_field]]\n",
        "\n",
        "\n",
        "# 您可以針對其他欄位類似地使用 stopwords_by_field 進行過濾\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Step 6: 檢查字體文件是否可用\n",
        "# -------------------------------\n",
        "# 此處使用您已確認正常的中文字體路徑\n",
        "font_path = '/content/NotoSansTC-Regular.ttf'\n",
        "try:\n",
        "    font = ImageFont.truetype(font_path, size=20)\n",
        "    print(\"字體加載成功\")\n",
        "except IOError:\n",
        "    print(\"字體加載失敗，請檢查文件是否正確\")\n",
        "    raise\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6zDIV_NvZKEM"
      },
      "outputs": [],
      "source": [
        "我做一個吃貪吃 # -------------------------------\n",
        "# Step 7: 生成文字雲\n",
        "# -------------------------------\n",
        "wc = WordCloud(\n",
        "    font_path=font_path,          # 指定中文字體\n",
        "    background_color=\"white\",     # 背景顏色\n",
        "    max_words=100,                # 最大顯示詞數\n",
        "    width=1000,                    # 圖片寬度\n",
        "    height=500                    # 圖片高度\n",
        ")\n",
        "\n",
        "# 將斷詞結果轉換為以空格分隔的字符串\n",
        "seg_text = ' '.join(seg_list)\n",
        "\n",
        "if seg_text.strip():\n",
        "    wc.generate(seg_text)\n",
        "    plt.figure(figsize=(40, 20),dpi=500)\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"斷詞結果為空，無法生成文字雲\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsJhVrvxR/hvXmxAWUGa3m"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}